# -*- coding: utf-8 -*- 
"""Conversational AI for PDF's streamlit/instructor-xl.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iA9HBNOLf_oMbJSmO-i0hpqPpLM7tU80
"""

!pip install streamlit
!pip install pypdf2
!pip install langchain
!pip install hugginface_hub

!pip install python-dotenv
!pip install pyngrok

!pip install faiss-gpu

! pip install InstructorEmbedding

!pip install sentence-transformers

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# 
# import streamlit as st
# from dotenv import load_dotenv
# from PyPDF2 import PdfReader
# from langchain.text_splitter import CharacterTextSplitter
# from langchain.embeddings import HuggingFaceInstructEmbeddings
# from InstructorEmbedding import INSTRUCTOR
# from langchain.vectorstores import FAISS
# from langchain.chat_models import ChatOpenAI
# from langchain.memory import ConversationBufferMemory
# from langchain.chains import ConversationalRetrievalChain
# from langchain.llms import HuggingFaceHub
# 
# def get_pdf_text(pdf_docs):
#     text = ""
#     for pdf in pdf_docs:
#         pdf_reader = PdfReader(pdf)
#         for page in pdf_reader.pages:
#             text += page.extract_text()
#     return text
# 
# def get_text_chunks(text):
#     text_splitter = CharacterTextSplitter(
#         separator="\n",
#         chunk_size=1000,
#         chunk_overlap=200,
#         length_function=len
#     )
#     chunks = text_splitter.split_text(text)
#     return chunks
# 
# def get_vectorstore(text_chunks):
#     embeddings = HuggingFaceInstructEmbeddings(model_name="hkunlp/instructor-large")
#     vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)
#     return vectorstore
# 
# def get_conversation_chain(vectorstore):
#     llm = HuggingFaceHub(repo_id="google/flan-t5-xxl", model_kwargs={"temperature":0.5, "max_length":512})
# 
#     memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)
#     conversation_chain = ConversationalRetrievalChain.from_llm(
#         llm=llm,
#         retriever=vectorstore.as_retriever(),
#         memory=memory
#     )
#     return conversation_chain
# 
# def css():
#   css = '''
#   <style>
#   .chat-message {
#       padding: 1.5rem; border-radius: 0.5rem; margin-bottom: 1rem; display: flex
#   }
#   .chat-message.user {
#       background-color: #2b313e
#   }
#   .chat-message.bot {
#       background-color: #475063
#   }
#   .chat-message .avatar {
#     width: 20%;
#   }
#   .chat-message .avatar img {
#      max-width: 78px;
#      max-height: 78px;
#      border-radius: 50%;
#      object-fit: cover;
#   }
#   .chat-message .message {
#     width: 80%;
#     padding: 0 1.5rem;
#     color: #fff;
#   }
# '''
# 
# def bot_template():
#   bot_template = '''
#   <div class="chat-message bot">
#       <div class="avatar">
#           <img src="https://i.ibb.co/cN0nmSj/Screenshot-2023-05-28-at-02-37-21.png" style="max-height: 78px; max-width: 78px; border-radius: 50%; object-fit: cover;">
#       </div>
#       <div class="message">{{MSG}}</div>
#   </div>
# '''
# 
# def user_template():
#   user_template = """
#   <div class="chat-message user">
#       <div class="avatar">
#           <img src="https://i.ibb.co/rdZC7LZ/Photo-logo-1.png">
#       </div>
#       <div class="message">{{MSG}}</div>
#   </div>
#   """
# 
# 
# 
# def handle_userinput(user_question):
#     response = st.session_state.conversation({'chat_history':user_question})
#     st.session_state.chat_history = response['chat_history']
# 
#     for i, message in enumerate(st.session_state.chat_history):
#         if i % 2 == 0:
#             st.write(user_template.replace(
#                 "{{MSG}}", message.content), unsafe_allow_html=True)
#         else:
#             st.write(bot_template.replace(
#                 "{{MSG}}", message.content), unsafe_allow_html=True)
# 
# def main():
#     load_dotenv()
#     st.set_page_config(page_title="Chat with multiple PDFs",
#                        page_icon=":books:")
#     st.write(css, unsafe_allow_html=True)
# 
#     if "conversation" not in st.session_state:
#         st.session_state.conversation = None
#     if "chat_history" not in st.session_state:
#         st.session_state.chat_history = None
# 
#     st.header("Chat with multiple PDFs :books:")
#     user_question = st.text_input("Ask a question about your documents:")
#     if user_question:
#         handle_userinput(user_question)
# 
#     with st.sidebar:
#         st.subheader("Your documents")
#         pdf_docs = st.file_uploader(
#             "Upload your PDFs here and click on 'Process'", accept_multiple_files=True)
#         if st.button("Process"):
#             with st.spinner("Processing"):
#                 # get pdf text
#                 raw_text = get_pdf_text(pdf_docs)
# 
#                 # get the text chunks
#                 text_chunks = get_text_chunks(raw_text)
# 
#                 # create vector store
#                 vectorstore = get_vectorstore(text_chunks)
# 
#                 # create conversation chain
#                 st.session_state.conversation = get_conversation_chain(
#                     vectorstore)
# 
# if __name__ == '__main__':
#     main()

!streamlit run app.py & npx localtunnel --port 8501

